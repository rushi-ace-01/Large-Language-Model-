# Large Language Model:
I gained a deep understanding of LLM and its construction from the book "Building a LLM from scratch"- by Sebastian Raschka
This repository contains code for building a LLM from scratch.
Code contains every detailed steps mentioned in the book.

A LLM is divide into three stages :

![llm](https://github.com/user-attachments/assets/48fdd0d5-f4d3-49dd-ae64-9f9f156f0c8d)


##Stage 1:
###Data preparatin and sampling:
1. Creating Tokens
2. Creating token IDS
3. Adding special context tokens
4. Byte Pair Encoding (BPE)
5. Creating Input- Target pairs
6. Implementing a Data loader
7. Creating token embeddings
8. Positional Embeddings

###Attention Mechanism:

![image](https://github.com/user-attachments/assets/f25bfff9-f136-4eee-98b5-c65a2a468a05)


1. Simplified Attention mechanism
2. Implementing self attention with trainable weights
3. Compact self attention
4. Casual attention
5. Multi head attention
